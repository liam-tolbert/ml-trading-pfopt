{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0c666ef-f0d9-49cd-ba39-af137767330a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-23T15:45:37.996037Z",
     "start_time": "2025-08-23T15:45:37.987710Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "# imports\n",
    "import numpy as np\n",
    "import shap\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.interchange.dataframe_protocol import DataFrame\n",
    "from statsmodels.tsa.regime_switching.markov_regression import MarkovRegression\n",
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "#from sympy.abc import lamda\n",
    "#from sympy.strategies.core import switch\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import indicators\n",
    "import importlib\n",
    "importlib.reload(indicators)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import OrderedDict"
   ],
   "id": "f0c666ef-f0d9-49cd-ba39-af137767330a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a30b8c33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:56:44.138025Z",
     "start_time": "2025-08-18T01:56:43.346745Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/100_stocks_sectors.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     30\u001b[39m real_stock_portfolio_3 = [\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mABNB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mACN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mALAB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAMZN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mANET\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAOSL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAPP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     32\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mASAN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mASML\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mAVGO\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBAH\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBITO\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBWXT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCLS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCOHR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCOIN\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCOST\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mVRT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWMT\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWRD\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mXYZ\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHIMS\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mOSCR\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m ]\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# One of my ideas was to take some stocks and put them into sectors to train their own models. Didn't really work out.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m sectors_tickers = pd.read_excel(\u001b[33m\"\u001b[39m\u001b[33mdata/100_stocks_sectors.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     45\u001b[39m healthcare = \u001b[38;5;28mtuple\u001b[39m(sectors_tickers[\u001b[33m\"\u001b[39m\u001b[33mHealthcare\u001b[39m\u001b[33m\"\u001b[39m].to_list())\n\u001b[32m     46\u001b[39m consumer_discretionary = \u001b[38;5;28mtuple\u001b[39m(sectors_tickers[\u001b[33m\"\u001b[39m\u001b[33mConsumer Discretionary\u001b[39m\u001b[33m\"\u001b[39m].to_list())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\ml-trading\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = ExcelFile(\n\u001b[32m    496\u001b[39m         io,\n\u001b[32m    497\u001b[39m         storage_options=storage_options,\n\u001b[32m    498\u001b[39m         engine=engine,\n\u001b[32m    499\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m    500\u001b[39m     )\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\ml-trading\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = inspect_excel_format(\n\u001b[32m   1551\u001b[39m         content_or_path=path_or_buffer, storage_options=storage_options\n\u001b[32m   1552\u001b[39m     )\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\ml-trading\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[32m   1403\u001b[39m     content_or_path, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m, storage_options=storage_options, is_text=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1404\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\miniconda3\\envs\\ml-trading\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/100_stocks_sectors.xlsx'"
     ]
    }
   ],
   "source": [
    "stocks = [\n",
    "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"META\", \"TSLA\", \"NVDA\", \"NFLX\", \"AMD\", \"INTC\",\n",
    "    \"JPM\", \"GS\", \"BAC\", \"C\", \"WFC\", \"V\", \"MA\", \"AXP\", \"BRK-B\",\n",
    "    \"UNH\", \"JNJ\", \"PFE\", \"LLY\", \"ABBV\", \"TMO\", \"DHR\", \"BMY\", \"GILD\",\n",
    "    \"XOM\", \"CVX\", \"COP\", \"OXY\", \"SLB\", \"HAL\", \"BP\", \"SHEL\", \"EOG\",\n",
    "    \"WMT\", \"COST\", \"HD\", \"LOW\", \"MCD\", \"SBUX\", \"TGT\", \"NKE\", \"PG\", \"KO\",\n",
    "    \"ADBE\", \"CRM\", \"ORCL\", \"QCOM\", \"AVGO\", \"TXN\", \"INTU\", \"CSCO\", \"IBM\", \"MU\",\n",
    "    \"PANW\", \"NOW\", \"CDNS\", \"ANET\", \"LRCX\", \"SNPS\", \"ACN\", \"FTNT\", \"MSI\", \"ZM\",\n",
    "    \"BLK\", \"TFC\", \"USB\", \"BK\", \"SCHW\", \"SPGI\", \"MS\", \"ICE\", \"PGR\", \"AIG\",\n",
    "    \"CB\", \"MMC\", \"MET\", \"ALL\", \"PRU\", \"CME\", \"COF\", \"DFS\", \"FITB\", \"MTB\",\n",
    "    \"ABT\", \"MRK\", \"ZBH\", \"ISRG\", \"MDT\", \"CVS\", \"CI\", \"REGN\", \"VRTX\", \"SYK\",\n",
    "    \"PSX\", \"MPC\", \"VLO\", \"KMI\", \"WMB\", \"CTRA\", \"DVN\", \"HES\", \"FANG\",\n",
    "    \"LIN\", \"APD\", \"ECL\", \"SHW\", \"NEM\", \"FCX\", \"DD\", \"ALB\", \"LYB\", \"MLM\",\n",
    "    \"PEP\", \"CL\", \"KMB\", \"EL\", \"MNST\", \"KR\", \"DG\", \"DLTR\", \"GIS\", \"MDLZ\",\n",
    "    \"TSCO\", \"ROST\", \"TJX\", \"YUM\", \"WBA\", \"PM\", \"MO\", \"UL\", \"HSY\", \"HRL\",\n",
    "    \"CAT\", \"DE\", \"HON\", \"LMT\", \"RTX\", \"BA\", \"GE\", \"NOC\", \"ETN\", \"EMR\",\n",
    "    \"UNP\", \"NSC\", \"FDX\", \"UPS\", \"CSX\", \"WM\", \"EXC\", \"DUK\", \"NEE\",\n",
    "    \"PLD\", \"AMT\", \"CCI\", \"EQIX\", \"SPG\", \"PSA\", \"O\", \"DLR\", \"VTR\", \"ARE\"\n",
    "]\n",
    "\n",
    "# Alternatively, use stocks2 if you want to\n",
    "stocks2 = [\n",
    "    \"ADBE\", \"CRM\", \"ORCL\", \"SAP\", \"NOW\", \"SHOP\", \"SQ\", \"ZM\", \"CRWD\", \"DDOG\",\n",
    "    \"TXN\", \"QCOM\", \"AVGO\", \"MU\", \"LRCX\", \"KLAC\", \"NXPI\", \"ADI\", \"MRVL\", \"SWKS\",\n",
    "    \"PYPL\", \"INTU\", \"FISV\", \"ADP\", \"VEEV\", \"TEAM\", \"WDAY\", \"ZS\", \"OKTA\", \"MDB\",\n",
    "    \"T\", \"VZ\", \"TMUS\", \"CHTR\", \"CMCSA\", \"DIS\", \"ROKU\", \"LYV\", \"TTWO\", \"ATVI\",\n",
    "    \"PEP\", \"KMB\", \"CL\", \"HSY\", \"MDLZ\", \"GIS\", \"MO\", \"PM\", \"EL\", \"STZ\"\n",
    "]\n",
    "\n",
    "real_stock_portfolio_3 = [\n",
    "    \"AAPL\", \"ABNB\", \"ACN\", \"ALAB\", \"AMD\", \"AMZN\", \"ANET\", \"AOSL\", \"APP\",\n",
    "    \"ASAN\", \"ASML\", \"AVGO\", \"BAH\", \"BITO\", \"BWXT\", \"CLS\", \"COHR\", \"COIN\", \"COST\",\n",
    "    \"COWG\", \"CPRX\", \"CRDO\", \"CRM\", \"CRWV\", \"DAVE\", \"DELL\", \"DKNG\", \"DOCS\",\n",
    "    \"DXPE\", \"EPD\", \"FBTC\", \"FVRR\", \"GOOG\", \"GRNY\", \"HOOD\", \"IHAK\", \"INTA\", \"IONQ\",\n",
    "    \"JPM\", \"LITE\", \"LQDT\", \"LUNR\", \"META\", \"MRVL\", \"MSFT\", \"MU\", \"NBIS\", \"NEE\",\n",
    "    \"NFLX\", \"NLR\", \"NNE\", \"NUTX\", \"NVDA\", \"NVDY\", \"NVO\", \"OUST\", \"OXY\", \"PANW\",\n",
    "    \"PEP\", \"PLD\", \"PLTR\", \"PYPL\", \"QCOM\", \"QTUM\", \"RBRK\", \"RDDT\", \"RDNT\", \"REAL\",\n",
    "    \"RGTI\", \"S\", \"SAIC\", \"SCHD\", \"SEZL\", \"SKYW\", \"SMCI\", \"SMTC\", \"SNOW\", \"SOXL\",\n",
    "    \"SYM\", \"TEAM\", \"TEM\", \"TOST\", \"TSM\", \"U\", \"UBER\", \"UPST\", \"URA\", \"VIST\",\n",
    "    \"VRT\", \"WMT\", \"WRD\", \"XYZ\", \"HIMS\", \"OSCR\"\n",
    "]\n",
    "\n",
    "# One of my ideas was to take some stocks and put them into sectors to train their own models. Didn't really work out.\n",
    "\"\"\"sectors_tickers = pd.read_excel(\"data/100_stocks_sectors.xlsx\")\n",
    "healthcare = tuple(sectors_tickers[\"Healthcare\"].to_list())\n",
    "consumer_discretionary = tuple(sectors_tickers[\"Consumer Discretionary\"].to_list())\n",
    "consumer_staples = tuple(sectors_tickers[\"Consumer Staples\"].to_list())\n",
    "communication = tuple(sectors_tickers[\"Communication\"].to_list())\n",
    "energy = tuple(sectors_tickers[\"Energy\"].to_list())\n",
    "financials = tuple(sectors_tickers[\"Financials\"].to_list())\n",
    "industrials = tuple(sectors_tickers[\"Industrials\"].to_list())\n",
    "technology = tuple(sectors_tickers[\"Tech\"].to_list())\n",
    "utilities = tuple(sectors_tickers[\"Utilities\"].to_list())\n",
    "real_estate = tuple(sectors_tickers[\"Real Estate\"].to_list())\n",
    "materials = tuple(sectors_tickers[\"Materials\"].to_list())\n",
    "overall_tickers = tuple(list(healthcare) + list(consumer_discretionary) + list(consumer_staples) + list(communication) + list(energy) + list(financials) + list(industrials) + list(technology) + list(utilities) + list(real_estate) + list(materials))\n",
    "\n",
    "\n",
    "sectors_dict = {\n",
    "    healthcare: \"Healthcare\",\n",
    "    consumer_discretionary: \"ConsumerDiscretionary\",\n",
    "    consumer_staples: \"ConsumerStaples\",\n",
    "    communication: \"Communication\",\n",
    "    energy: \"Energy\",\n",
    "    financials: \"Financials\",\n",
    "    industrials: \"Industrials\",\n",
    "    technology: \"Technology\",\n",
    "    utilities: \"Utilities\",\n",
    "    real_estate: \"RealEstate\",\n",
    "    materials: \"Materials\"\n",
    "}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2bd7b3d7001517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this and/or the next cell if you want to refresh the dataset(s)\n",
    "#x = yf.download(stocks, start=\"2015-01-01\", end=\"2025-04-15\", interval=\"1wk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba74d74e0fa06f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sp500 = yf.download(\"^GSPC\", start=\"2015-01-01\", interval=\"1wk\")\n",
    "\n",
    "sp500.columns = [\"_\".join(col) if isinstance(col, tuple) else col for col in sp500.columns]\n",
    "\n",
    "sp500 = sp500.loc[:, ~sp500.columns.isin([\"level_0\", \"index\"])]\n",
    "sp500.to_csv(\"SP500.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe107e02e4c1b3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:56:48.851905Z",
     "start_time": "2025-08-18T01:56:48.841571Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_and_fix_yfinance_data(stocks, csv_filename): # Takes raw yfinance dataframe and fixes it if it has that weird date syncing problem\n",
    "    data = {}\n",
    "\n",
    "    for t in stocks:\n",
    "        df = yf.download(t, interval='1d', start='2015-01-01', auto_adjust=True)\n",
    "        df = df.resample('W-FRI').agg({ # everything needs to be on the friday interval\n",
    "          ('Open', t): 'first',\n",
    "          ('High', t): 'max',\n",
    "          ('Low', t): 'min',\n",
    "          ('Close', t): 'last',\n",
    "          ('Volume', t): 'sum'\n",
    "        })\n",
    "        df.name = t\n",
    "        data[t] = df\n",
    "\n",
    "    # Combine and align on the same date index\n",
    "    combined = pd.concat(data.values(), axis=1, join='outer')\n",
    "    combined = combined.ffill()  # or .bfill(), depending on your use case\n",
    "\n",
    "    non_nan_counts = combined.count()\n",
    "    cols_to_drop = non_nan_counts[non_nan_counts < 20].index\n",
    "    y = combined.drop(columns=cols_to_drop)\n",
    "\n",
    "    stockData = y.copy()\n",
    "\n",
    "    # Rename columns, joining multi-level column names into a single string with \"_\".\n",
    "    stockData.columns = [\"_\".join(col) if isinstance(col, tuple) else col for col in stockData.columns]\n",
    "\n",
    "    # Remove unnecessary columns such as \"level_0\" or \"index\" that may have been carried over.\n",
    "    stockData = stockData.loc[:, ~stockData.columns.isin([\"level_0\", \"index\"])]\n",
    "    stockData.to_csv(csv_filename + \".csv\")\n",
    "\n",
    "#for sector_list, sector_name in sectors_dict.items():\n",
    "#  download_and_fix_yfinance_data(sector_list, sector_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0f9df2578bc212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:56:51.816313Z",
     "start_time": "2025-08-18T01:56:51.809483Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_and_fix_sp500():\n",
    "    y = yf.download(\"^GSPC\", start=\"2015-01-01\", interval=\"1d\")\n",
    "    y = y.resample('W-FRI').agg({\n",
    "          ('Open', \"^GSPC\"): 'first',\n",
    "          ('High', \"^GSPC\"): 'max',\n",
    "          ('Low', \"^GSPC\"): 'min',\n",
    "          ('Close', \"^GSPC\"): 'last',\n",
    "          ('Volume', \"^GSPC\"): 'sum'\n",
    "        })\n",
    "    sp500 = y.copy()\n",
    "\n",
    "    sp500.columns = [\"_\".join(col) if isinstance(col, tuple) else col for col in sp500.columns]\n",
    "\n",
    "    sp500 = sp500.loc[:, ~sp500.columns.isin([\"level_0\", \"index\"])]\n",
    "    sp500.to_csv(\"SP500.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0166c29b0c3b63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:56:55.289068Z",
     "start_time": "2025-08-18T01:56:55.255515Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_ticker_dataframe(csv_filepath: str, ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a multi-ticker CSV file (like one from yfinance) and isolates the data\n",
    "    for the specified ticker. This updated version assumes that the CSV contains the\n",
    "    dates as the index (first column), so we use that as the Date information.\n",
    "\n",
    "    The CSV is expected to have a two-row header:\n",
    "      - The first row contains field names (e.g., \"Open\", \"High\", etc.).\n",
    "      - The second row contains the ticker symbols for each column.\n",
    "\n",
    "    The returned DataFrame's columns will be ordered as needed for Backtrader:\n",
    "      Date, Open, High, Low, Close, Volume.\n",
    "    \"\"\"\n",
    "    # Use the first column as the index so that the dates are read from the CSV.\n",
    "    df = pd.read_csv(csv_filepath)#, header=[0, 1], index_col=0)\n",
    "\n",
    "    # Convert the index to datetime in case it's not already\n",
    "    df.index = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "    df = df.drop(\"Date\", axis=1)\n",
    "\n",
    "    # Identify all columns for the specified ticker (matching on the lower level).\n",
    "    ticker_cols = [col for col in df.columns if ticker == str(col).strip().split(\"_\")[-1]]\n",
    "    if not ticker_cols:\n",
    "        raise ValueError(f\"Ticker '{ticker}' not found in the CSV file.\")\n",
    "\n",
    "    # Extract the ticker's columns\n",
    "    df_ticker = df.loc[:, ticker_cols].copy()\n",
    "    #df_ticker.columns = [col[0].strip() for col in df_ticker.columns]\n",
    "\n",
    "    # Removing ticker name from column names\n",
    "    for col in df_ticker.columns:\n",
    "        df_ticker.rename(columns={col: str(col).split(\"_\")[0]}, inplace=True)\n",
    "\n",
    "    desired_order = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "\n",
    "    available_order = [col for col in desired_order if col in df_ticker.columns]\n",
    "    df_ticker = df_ticker[available_order]\n",
    "\n",
    "    df_ticker = df_ticker.reset_index().rename(columns={\"index\": \"Date\"}).set_index(\"Date\")\n",
    "\n",
    "    return df_ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20041a3b4c31a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you uncommented the yf.download cells, uncomment these as well. One is for the stocks and the other is for the sp500\n",
    "'''\n",
    "stockData = x.copy()\n",
    "\n",
    "# Rename columns, joining multi-level column names into a single string with \"_\".\n",
    "stockData.columns = [\"_\".join(col) if isinstance(col, tuple) else col for col in stockData.columns]\n",
    "\n",
    "# Remove unnecessary columns such as \"level_0\" or \"index\" that may have been carried over.\n",
    "stockData = stockData.loc[:, ~stockData.columns.isin([\"level_0\", \"index\"])]\n",
    "stockData.to_csv(\"50stocks.csv\")\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "sp500 = y.copy()\n",
    "\n",
    "sp500.columns = [\"_\".join(col) if isinstance(col, tuple) else col for col in sp500.columns]\n",
    "\n",
    "sp500 = sp500.loc[:, ~sp500.columns.isin([\"level_0\", \"index\"])]\n",
    "sp500.to_csv(\"SP500.csv\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3d74d-2b42-4d24-ac2f-787adac5c64c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:56:58.588486Z",
     "start_time": "2025-08-18T01:56:58.551410Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Feature engineering: Hamilton Regime Switching Model\n",
    "sp500 = pd.read_csv(\"data/SP500.csv\")\n",
    "sp500.set_index(\"Date\", inplace=True)\n",
    "sp500.index = pd.to_datetime(sp500.index)\n",
    "sp500['Log_Returns'] = np.log(sp500['Close_^GSPC'] / sp500['Close_^GSPC'].shift(1))\n",
    "sp500['Returns_6wk'] = sp500['Close_^GSPC'].pct_change(6)\n",
    "sp500 = sp500.dropna()\n",
    "\n",
    "def classify_regimes(sp500: pd.DataFrame) -> pd.DataFrame:\n",
    "    model = MarkovRegression(sp500['Log_Returns'], k_regimes=2, trend='c', switching_variance=True)\n",
    "    result = model.fit()\n",
    "    #print(result.summary())\n",
    "    smoothed_probs = result.smoothed_marginal_probabilities\n",
    "    sp500['Regime'] = smoothed_probs.idxmax(axis=1)\n",
    "    sp500['Bull_Prob'] = smoothed_probs[0]\n",
    "\n",
    "    \"\"\"if show_regimes:\n",
    "        plt.plot(sp500.index, smoothed_probs[0], label=\"Probability of Bull Market\")\n",
    "        plt.fill_between(sp500.index, 0, 1, where=sp500['Bull_Prob'] > 0.5, color='green', alpha=0.3)\n",
    "        plt.fill_between(sp500.index, 0, 1, where=sp500['Bull_Prob'] <= 0.5, color='red', alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.title(\"Bull vs. Bear Market Probability\")\n",
    "        plt.show()\"\"\"\n",
    "\n",
    "    return sp500[\"Bull_Prob\"].to_frame()\n",
    "    # 0 -> Bull, 1 -> Bear\n",
    "\n",
    "def compute_beta(asset_returns, market_returns, window=20):\n",
    "    cov = asset_returns.rolling(window).cov(market_returns)\n",
    "    var = market_returns.rolling(window).var()\n",
    "    return cov / var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab565fc88c678dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T01:57:02.145814Z",
     "start_time": "2025-08-18T01:57:02.137760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute rolling portfolio weights using a lookback period (e.g., 52 weeks)\n",
    "def compute_rolling_portfolio_weights(data, lookback_window=52):\n",
    "    \"\"\"\n",
    "    Computes portfolio weights for each date using historical data up to that date.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame with dates as index and stocks as columns.\n",
    "        lookback_window (int): Number of days to look back for the optimization.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with dates as index and stocks as columns, containing weights.\n",
    "    \"\"\"\n",
    "    weights_list = []\n",
    "    dates = []\n",
    "    for date in data.index[lookback_window:]:\n",
    "        window_data = data.loc[:date].tail(lookback_window)\n",
    "        mu = expected_returns.mean_historical_return(window_data)\n",
    "        S = risk_models.sample_cov(window_data)\n",
    "        ef = EfficientFrontier(mu, S)\n",
    "        try:\n",
    "            ef.max_sharpe()\n",
    "            clean_weights = ef.clean_weights()\n",
    "        except Exception as e:\n",
    "            # In case optimization fails, assign zero weights.\n",
    "            clean_weights = {stock: 0 for stock in data.columns}\n",
    "        weights_list.append(clean_weights)\n",
    "        dates.append(date)\n",
    "    weights_df = pd.DataFrame(weights_list, index=dates)\n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a7cf2f-ba18-4c0b-9d54-b241ce9e87b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:01:54.490102Z",
     "start_time": "2025-08-18T01:57:03.972144Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Feature engineering: Technical indicators, fundamentals\n",
    "# SMA, RSI, MACD, etc. etc. basically, seeing which indicator sticks\n",
    "def create_stock_features(stocks, stock_data_filename) -> pd.DataFrame:\n",
    "    feature_rows = []\n",
    "    regime_df = classify_regimes(sp500)\n",
    "\n",
    "    for stock in stocks:\n",
    "        # will be filled with indicators for one stock\n",
    "        try:\n",
    "            prices = extract_ticker_dataframe(stock_data_filename, stock)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        features = pd.DataFrame(index=prices.index)\n",
    "\n",
    "        # Simple Moving Avg Comparison (5 vs 20)\n",
    "        sma = indicators.sma_strategy(prices[\"Close\"].to_frame(), 5, 20)\n",
    "        features[\"SMA_5v20\"] = sma[\"signal_raw\"]\n",
    "\n",
    "        # Volume\n",
    "        features[\"Volume\"] = prices[\"Volume\"]\n",
    "\n",
    "        # Relative Strength Index (RSI)\n",
    "        rsi = indicators.rsi_strategy(prices[\"Close\"].to_frame())\n",
    "        features[\"RSI\"] = rsi[\"rsi\"]\n",
    "\n",
    "        # Moving Average Convergence Divergence (MACD)\n",
    "        macd = indicators.macd_strategy(prices[\"Close\"].to_frame())\n",
    "        features[\"MACD\"] = macd[\"signal_raw\"] # only using signal, not other columns, since I don't want to have weird \"fitting\" of the model to the components of the macd signal\n",
    "\n",
    "        # Bollinger Bands\n",
    "        bands = indicators.bollinger_strategy(prices[\"Close\"].to_frame())\n",
    "        features[\"Bollinger_Bands\"] = bands[\"signal_ternary\"]\n",
    "\n",
    "        # Average True Range (ATR)\n",
    "        atr = indicators.atr_indicator(prices[[\"High\", \"Low\", \"Close\"]])\n",
    "        features[\"ATR\"] = atr[\"signal\"]\n",
    "\n",
    "        # Stochastic Oscillator Strategy\n",
    "        stochastic = indicators.stochastic_strategy(prices[[\"High\", \"Low\", \"Close\"]])\n",
    "        features[\"Stochastic\"] = stochastic[\"signal\"]\n",
    "\n",
    "        # OBV (On-Balance Volume)\n",
    "        obv = indicators.obv_strategy(prices[[\"Close\", \"Volume\"]])\n",
    "        features[\"OBV\"] = obv[\"obv\"]\n",
    "\n",
    "        # ADX (Average Directional Index)\n",
    "        adx = indicators.adx_strategy(prices[[\"High\", \"Low\", \"Close\"]])\n",
    "        features[\"ADX\"] = adx[\"adx\"]\n",
    "\n",
    "        # Aroon Indicator\n",
    "        aroon = indicators.aroon_strategy(prices[[\"High\", \"Low\"]])\n",
    "        features[\"Aroon\"] = aroon[\"aroon_oscillator\"]\n",
    "\n",
    "        # Returns features. Overall 4 week percent change, split into 3 week period, 1 week lag and 1 week period, 0 week lag\n",
    "        features[\"Returns-3wk-1wklag\"] = prices[\"Close\"].pct_change(periods=3).shift(1)\n",
    "        features[\"Returns-1wk-0wklag\"] = prices[\"Close\"].pct_change()\n",
    "\n",
    "        # Other technical indicators and fundamentals?\n",
    "\n",
    "        # rolling return (2 week window)\n",
    "        #prices[\"Returns-2wk\"] = prices[\"Close\"].pct_change(periods=2)\n",
    "        #features[\"Returns-2wk\"] = prices[\"Returns-2wk\"]\n",
    "\n",
    "        # future return (for labels)\n",
    "        features['Returns-future-1wk'] = prices['Close'].pct_change(periods=1).shift(-1)\n",
    "        features['Returns-future-2wk'] = prices['Close'].pct_change(periods=2).shift(-2)\n",
    "\n",
    "        features[\"Bull_Probability\"] = regime_df[\"Bull_Prob\"]\n",
    "\n",
    "        # Volatility features\n",
    "        features['SP500-Log-Returns'] = sp500['Log_Returns']\n",
    "        features['SP500-Returns'] = sp500['Returns_6wk']\n",
    "\n",
    "        prices['Log_Returns'] = np.log(prices['Close'] / prices['Close'].shift(1))\n",
    "\n",
    "        prices['Volatility_4wk'] = prices['Log_Returns'].rolling(window=4).std()\n",
    "        prices['Volatility_12wk'] = prices['Log_Returns'].rolling(window=12).std()\n",
    "        features['Volatility_Spike'] = prices['Volatility_4wk'] / prices['Volatility_12wk']\n",
    "\n",
    "        features['Beta_26wk'] = compute_beta(prices['Log_Returns'], sp500['Log_Returns'])\n",
    "\n",
    "        # Time information\n",
    "        features['Week-of-year'] = prices.index.isocalendar().week\n",
    "        features['Month-of-year'] = prices.index.month\n",
    "        features['Quarter'] = prices.index.quarter\n",
    "\n",
    "        # Year of presidential term\n",
    "        features['Presidential-year'] = prices.index.to_series().apply(presidential_term_year)\n",
    "\n",
    "\n",
    "        features[\"Stock\"] = stock\n",
    "        features = features.reset_index().rename(columns={\"index\": \"Date\"})\n",
    "        feature_rows.append(features)\n",
    "\n",
    "    # Combine data for all stocks into one dataframe\n",
    "    features_long = pd.concat(feature_rows, ignore_index=True).set_index(\"Date\").dropna()\n",
    "    return features_long\n",
    "\n",
    "def presidential_term_year(date):\n",
    "    # Start years of presidential terms\n",
    "    election_years = [2000, 2004, 2008, 2012, 2016, 2020, 2024]\n",
    "    year = date.year\n",
    "    for start in reversed(election_years):\n",
    "        if year >= start:\n",
    "            return (year - start) + 1  # 1 to 4\n",
    "    return np.nan\n",
    "\n",
    "overall_df = create_stock_features(overall_tickers, \"data/training_set.csv\")\n",
    "\"\"\"communication_df = create_stock_features(communication, \"data/Communication.csv\")\n",
    "consumer_staples_df = create_stock_features(consumer_staples, \"data/ConsumerStaples.csv\")\n",
    "consumer_discretionary_df = create_stock_features(consumer_discretionary, \"data/ConsumerDiscretionary.csv\")\n",
    "energy_df = create_stock_features(energy, \"data/Energy.csv\")\n",
    "financials_df = create_stock_features(financials, \"data/Financials.csv\")\n",
    "healthcare_df = create_stock_features(healthcare, \"data/Healthcare.csv\")\n",
    "industrials_df = create_stock_features(industrials, \"data/Industrials.csv\")\n",
    "real_estate_df = create_stock_features(real_estate, \"data/RealEstate.csv\")\n",
    "technology_df = create_stock_features(technology, \"data/Technology.csv\")\n",
    "utilities_df = create_stock_features(utilities, \"data/Utilities.csv\")\n",
    "materials_df = create_stock_features(materials, \"data/Materials.csv\")\"\"\""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def buy_low_sell_high_strategy(row):\n",
    "    returns_prev_week = row['Returns-1wk-0wklag']\n",
    "    if returns_prev_week < 0:\n",
    "        return 1 # Buy\n",
    "    else:\n",
    "        return 0 # Sell\n",
    "\n",
    "def buy_low_sell_high_model(df):\n",
    "    df['Signal'] = df.apply(buy_low_sell_high_strategy, axis=1)\n",
    "    df = df.sort_values(by='Date')\n",
    "    return df\n",
    "\n",
    "df = overall_df.copy()\n",
    "df = buy_low_sell_high_model(df) # df now contains the naive model's actions, includes features, though. When we use backtrader, we will probably get rid of those. "
   ],
   "id": "282d6d81538e270"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548eac63-4026-4551-b895-c9bf5d8afad3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:04:05.321708Z",
     "start_time": "2025-08-18T02:04:05.312285Z"
    }
   },
   "outputs": [],
   "source": [
    "def label_signal2(row, buy_thresh=0.01, sell_thresh=-0.015, vol_thresh=0.02):\n",
    "    r1 = row['Returns-future-1wk']\n",
    "    r2 = row['Returns-future-2wk']\n",
    "    if r1 < -0.01 and r2 < -0.015:\n",
    "        return 0  # Sell\n",
    "    elif r1 > 0.01 and r2 > 0.015:\n",
    "        return 1  # Buy\n",
    "    else:\n",
    "        return 2  # Hold\n",
    "\n",
    "def label_signal(row):\n",
    "    r1 = row['Returns-future-1wk']\n",
    "    r2 = row['Returns-future-2wk']\n",
    "    if r1 < -0.02 and r2 < -0.03:\n",
    "        return 0  # Strong Sell\n",
    "    elif r1 < -0.01 and r2 < -0.02:\n",
    "        return 1  # Weak Sell\n",
    "    elif r1 > 0.02 and r2 > 0.03:\n",
    "        return 4  # Strong Buy\n",
    "    elif r1 > 0.01 and r2 > 0.02:\n",
    "        return 3  # Weak Buy\n",
    "    else:\n",
    "        return 2  # Hold\n",
    "\n",
    "\n",
    "def train_val_test_split(df):\n",
    "    df['Signal'] = df.apply(label_signal2, axis=1)\n",
    "    df = df.dropna(subset=['Returns-future-1wk', 'Returns-future-2wk', 'Signal']) # dropping rows with no signal\n",
    "\n",
    "    # Sort by date?\n",
    "    df = df.sort_values(by='Date')\n",
    "\n",
    "    # 70% train, 15% val, 15% test\n",
    "    split_1 = int(len(df) * 0.7)\n",
    "    split_2 = int(len(df) * 0.85)\n",
    "\n",
    "    train = df.iloc[:split_1]\n",
    "    val = df.iloc[split_1:split_2]\n",
    "    test = df.iloc[split_2:]\n",
    "\n",
    "    return train, val, test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447ec4f593bf2af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:04:09.491503Z",
     "start_time": "2025-08-18T02:04:09.478939Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(sets: tuple):\n",
    "    train = sets[0]\n",
    "    features = train.drop(['Stock', 'Signal', 'Returns-future-1wk', 'Returns-future-2wk'], axis=1).columns.values\n",
    "    X_train = train[features]\n",
    "    y_train = train['Signal']\n",
    "\n",
    "    # balanced class weights to balance any over-representation\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    print(\"Class Weights:\", class_weight_dict)\n",
    "\n",
    "    sample_weights = y_train.map(class_weight_dict)\n",
    "\n",
    "    # Train the model\n",
    "    model = XGBClassifier(\n",
    "        objective='multi:softprob',  # for multi-class\n",
    "        num_class=len(classes),\n",
    "        eval_metric='mlogloss',\n",
    "        tree_method='hist',\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_lambda=5,\n",
    "        reg_alpha=5,\n",
    "        booster='gbtree',\n",
    "        learning_rate=0.01,\n",
    "        gamma=0.5,\n",
    "        min_child_weight=5,\n",
    "        n_estimators=200,\n",
    "        use_label_encoder=False,\n",
    "        random_state=42,\n",
    "        max_depth = 12,\n",
    "    )\n",
    "    model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "def predict(model: XGBClassifier, sets: tuple, val_or_test: str):\n",
    "    val = sets[1]\n",
    "    test = sets[2]\n",
    "    features = val.drop(['Stock', 'Signal', 'Returns-future-1wk', 'Returns-future-2wk'], axis=1).columns.values\n",
    "\n",
    "    if val_or_test == 'val':\n",
    "        x = val[features]\n",
    "        y = val['Signal']\n",
    "    else:\n",
    "        x = test[features]\n",
    "        y = test['Signal']\n",
    "\n",
    "    y_pred = model.predict(x)\n",
    "    return classification_report(y, y_pred, output_dict=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55679e6be149d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:04:27.242719Z",
     "start_time": "2025-08-18T02:04:16.964505Z"
    }
   },
   "outputs": [],
   "source": [
    "overall_sets = train_val_test_split(overall_df)\n",
    "\"\"\"communication_sets = train_val_test_split(communication_df)\n",
    "energy_sets = train_val_test_split(energy_df)\n",
    "consumer_discretionary_sets = train_val_test_split(consumer_discretionary_df)\n",
    "consumer_staples_sets = train_val_test_split(consumer_staples_df)\n",
    "financials_sets = train_val_test_split(financials_df)\n",
    "healthcare_sets = train_val_test_split(healthcare_df)\n",
    "industrials_sets = train_val_test_split(industrials_df)\n",
    "real_estate_sets = train_val_test_split(real_estate_df)\n",
    "technology_sets = train_val_test_split(technology_df)\n",
    "materials_sets = train_val_test_split(materials_df)\n",
    "utilities_sets = train_val_test_split(utilities_df)\"\"\"\n",
    "\n",
    "overall_model = train_model(overall_sets)\n",
    "\"\"\"communication_model = train_model(communication_sets)\n",
    "energy_model = train_model(energy_sets)\n",
    "consumer_discretionary_model = train_model(consumer_discretionary_sets)\n",
    "consumer_staples_model = train_model(consumer_staples_sets)\n",
    "financials_model = train_model(financials_sets)\n",
    "healthcare_model = train_model(healthcare_sets)\n",
    "industrials_model = train_model(industrials_sets)\n",
    "real_estate_model = train_model(real_estate_sets)\n",
    "technology_model = train_model(technology_sets)\n",
    "materials_model = train_model(materials_sets)\n",
    "utilities_model = train_model(utilities_sets)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c7d27a90b1e5c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:08:25.501766Z",
     "start_time": "2025-08-18T02:08:25.271744Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_overall = predict(overall_model, overall_sets, val_or_test='val')\n",
    "\"\"\"predict_cm = predict(communication_model, communication_sets, val_or_test='val')\n",
    "predict_e = predict(energy_model, energy_sets, val_or_test='val')\n",
    "predict_cd = predict(consumer_discretionary_model, consumer_discretionary_sets, val_or_test='val')\n",
    "predict_cs = predict(consumer_staples_model, consumer_staples_sets, val_or_test='val')\n",
    "predict_f = predict(financials_model, financials_sets, val_or_test='val')\n",
    "predict_h = predict(healthcare_model, healthcare_sets, val_or_test='val')\n",
    "predict_i = predict(industrials_model, industrials_sets, val_or_test='val')\n",
    "predict_re = predict(real_estate_model, real_estate_sets, val_or_test='val')\n",
    "predict_t = predict(technology_model, technology_sets, val_or_test='val')\n",
    "predict_m = predict(materials_model, materials_sets, val_or_test='val')\n",
    "predict_u = predict(utilities_model, utilities_sets, val_or_test='val')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9c1b015a88ddb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-18T02:08:48.815920Z",
     "start_time": "2025-08-18T02:08:48.809507Z"
    }
   },
   "outputs": [],
   "source": [
    "predict_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50510b9ae96d22bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T02:14:00.056200Z",
     "start_time": "2025-08-13T02:13:59.988475Z"
    }
   },
   "outputs": [],
   "source": [
    "importance = overall_model.get_booster().get_score(importance_type='weight')  # or 'gain', 'cover'\n",
    "importance2 = overall_model.get_booster().get_score(importance_type='gain')  # or 'gain', 'cover'\n",
    "importance3 = overall_model.get_booster().get_score(importance_type='cover')  # or 'gain', 'cover'\n",
    "\n",
    "print(importance2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a489d1c785eeaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 700],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 10, 12],\n",
    "    'min_child_weight': [1, 3, 5, 7, 10],\n",
    "    'gamma': [0, 0.1, 0.25, 0.5, 1],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5, 1, 5, 10],\n",
    "    'reg_lambda': [0, 0.1, 1, 5, 10, 20],\n",
    "    'booster': ['gbtree'],  # Could add 'dart' if you're interested in dropout boosting\n",
    "    'tree_method': ['hist'],  # 'hist' for faster CPU training; use 'gpu_hist' if on GPU\n",
    "    'objective': ['multi:softprob'],\n",
    "    'num_class': [3],  # Set according to your number of classes\n",
    "}\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    random_state=42,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=75,\n",
    "    scoring='f1_macro',  # or 'accuracy', or 'roc_auc_ovr'\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "#print(\"Best Params:\", search.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7668cc4941ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"booster = model.get_booster()\n",
    "explainer = shap.TreeExplainer(booster)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Initialize the SHAP JavaScript library\n",
    "shap.initjs()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112aead2365fbb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "#shap.summary_plot(shap_values[:, :, 0], X_test) # buy\n",
    "#shap.summary_plot(shap_values[:, :, 1], X_test) # hold\n",
    "shap.summary_plot(shap_values[:, :, 2], X_test) # sell\n",
    "shap.dependence_plot(\"Bull_Probability\", shap_values[:, :, 2], X_test)\n",
    "shap.dependence_plot(\"ATR\", shap_values[:, :, 2], X_test)\n",
    "shap.dependence_plot(\"MACD\", shap_values[:, :, 2], X_test)\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d21882d807b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing with a real portfolio\n",
    "real_stock_portfolio_1 = [\n",
    "    \"AMD\", \"NVDA\", \"RDDT\", \"SMCI\"\n",
    "]\n",
    "\n",
    "real_stock_portfolio_2 = [\n",
    "    \"CRDO\", \"GOOG\", \"APP\", \"ASAN\", \"CLS\", \"META\", \"QQQ\", \"LUNR\", \"JPM\", \"PANW\", \"HOOD\", \"SPY\", \"SKYW\", \"TSM\", \"UBER\", \"VRT\", \"WMT\", \"NLR\", \"URA\", \"BITO\", \"COST\", \"XYZ\"\n",
    "]\n",
    "\n",
    "real_stock_portfolio_3 = [\n",
    "    \"AAPL\", \"ABNB\", \"ACN\", \"ALAB\", \"AMD\", \"AMZN\", \"ANET\", \"AOSL\", \"APP\",\n",
    "    \"ASAN\", \"ASML\", \"AVGO\", \"BAH\", \"BITO\", \"BWXT\", \"CLS\", \"COHR\", \"COIN\", \"COST\",\n",
    "    \"COWG\", \"CPRX\", \"CRDO\", \"CRM\", \"CRWV\", \"DAVE\", \"DELL\", \"DKNG\", \"DOCS\",\n",
    "    \"DXPE\", \"EPD\", \"FBTC\", \"FVRR\", \"GOOG\", \"GRNY\", \"HOOD\", \"IHAK\", \"INTA\", \"IONQ\",\n",
    "    \"JPM\", \"LITE\", \"LQDT\", \"LUNR\", \"META\", \"MRVL\", \"MSFT\", \"MU\", \"NBIS\", \"NEE\",\n",
    "    \"NFLX\", \"NLR\", \"NNE\", \"NUTX\", \"NVDA\", \"NVDY\", \"NVO\", \"OUST\", \"OXY\", \"PANW\",\n",
    "    \"PEP\", \"PLD\", \"PLTR\", \"PYPL\", \"QCOM\", \"QTUM\", \"RBRK\", \"RDDT\", \"RDNT\", \"REAL\",\n",
    "    \"RGTI\", \"S\", \"SAIC\", \"SCHD\", \"SEZL\", \"SKYW\", \"SMCI\", \"SMTC\", \"SNOW\", \"SOXL\",\n",
    "    \"SYM\", \"TEAM\", \"TEM\", \"TOST\", \"TSM\", \"U\", \"UBER\", \"UPST\", \"URA\", \"VIST\",\n",
    "    \"VRT\", \"WMT\", \"WRD\", \"XYZ\", \"HIMS\", \"OSCR\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9904cca1e7d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T05:44:12.836256Z",
     "start_time": "2025-08-12T05:43:52.060828Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_model(stock_portfolio):\n",
    "    # Part 1: Getting recommendations\n",
    "    #download_and_fix_yfinance_data(stock_portfolio)\n",
    "    real_df = create_stock_features(stock_portfolio, \"data/stocks.csv\")\n",
    "\n",
    "    real_df = real_df.sort_values(by='Date')\n",
    "    today_stocks_features = real_df.loc[[real_df.index.max()]]\n",
    "\n",
    "    features = real_df.drop(['Stock', 'Returns-future-1wk', 'Returns-future-2wk'], axis=1).columns.values\n",
    "    model = overall_model#XGBClassifier()\n",
    "\n",
    "    \"\"\"for t in stock_portfolio:\n",
    "        info = yf.Ticker(t).info\n",
    "        try:\n",
    "            sector = info['sector']\n",
    "        except KeyError:\n",
    "            sector = 'default' # probably an etf or some other fund, not a stock\n",
    "\n",
    "        match sector:\n",
    "            case \"Communication Services\":\n",
    "                model = communication_model\n",
    "            case \"Consumer Discretionary\":\n",
    "                model = consumer_discretionary_model\n",
    "            case \"Consumer Staples\":\n",
    "                model = consumer_staples_model\n",
    "            case \"Energy\":\n",
    "                model = energy_model\n",
    "            case \"Financials\":\n",
    "                model = financials_model\n",
    "            case \"Health Care\":\n",
    "                model = healthcare_model\n",
    "            case \"Industrials\":\n",
    "                model = industrials_model\n",
    "            case \"Technology\":\n",
    "                model = technology_model\n",
    "            case \"Materials\":\n",
    "                model = materials_model\n",
    "            case \"Real Estate\":\n",
    "                model = real_estate_model\n",
    "            case \"Utilities\":\n",
    "                model = utilities_model\n",
    "            case _:\n",
    "                model = overall_model\"\"\"\n",
    "\n",
    "    todays_pred = model.predict(today_stocks_features[features])\n",
    "    todays_probs = model.predict_proba(today_stocks_features[features])  # shape: (n_samples, 3)\n",
    "    stock_col = today_stocks_features[\"Stock\"].reset_index().drop(columns=\"Date\")\n",
    "    pred_col = pd.DataFrame(todays_pred)\n",
    "    probs_col = pd.DataFrame(todays_probs)\n",
    "\n",
    "    # Making recommendations per stock\n",
    "    recommendations = stock_col.join(pred_col)\n",
    "    recommendations.columns = [\"Stock\", \"Recommendation\"]\n",
    "    recommendations[\"Recommendation\"] = recommendations[\"Recommendation\"].map({0: 'Sell', 1: 'Buy', 2: 'Hold'})\n",
    "\n",
    "    #recommendations = stock_col.join(probs_col)\n",
    "    #recommendations[\"Recommendation\"] = recommendations[\"Recommendation\"].map({0: 'Strong Sell', 1: 'Weak Sell', 2: 'Hold', 3: 'Weak Buy', 4: 'Strong Buy'})\n",
    "\n",
    "    # Part 2: Getting the Markowitz mean-variance portfolio\n",
    "\n",
    "    # Isolating buys\n",
    "    \"\"\"buy_recommendations = recommendations[recommendations.Recommendation == \"Buy\"].drop(columns=\"Recommendation\")\n",
    "    rec_array = buy_recommendations.to_numpy().flatten().tolist()\n",
    "    buy_stocks_history = extract_ticker_dataframe(\"stocks.csv\", rec_array[0])[\"Close\"]\n",
    "    for i in range(1, len(rec_array)):\n",
    "        a = extract_ticker_dataframe(\"stocks.csv\", rec_array[i])[\"Close\"]\n",
    "        buy_stocks_history = pd.concat([buy_stocks_history, a], axis=1, join='inner')\n",
    "    buy_stocks_history.columns = rec_array\n",
    "\n",
    "    def compute_adjusted_mu(buy_probs, baseline_mu, alpha=0.01):\n",
    "        tickers = baseline_mu.index.intersection(buy_probs.index)\n",
    "        adjusted_mu = baseline_mu.loc[tickers] * (1 + alpha * (buy_probs.loc[tickers] - 0.5))\n",
    "        return adjusted_mu\n",
    "\n",
    "    probs = model.predict_proba(today_stocks_features[features])\n",
    "    probs_db = pd.DataFrame(probs, columns=[\"Hold\", \"Buy\", \"Sell\"], index=today_stocks_features[\"Stock\"].values)\n",
    "    buy_probs = probs_db[\"Buy\"]\n",
    "\n",
    "    baseline_mu = expected_returns.mean_historical_return(buy_stocks_history)\n",
    "\n",
    "    # Alpha is a strength parameter for the adjustment. Larger values will make the adjustment more aggressive.\n",
    "    adjusted_mu = compute_adjusted_mu(buy_probs, baseline_mu, alpha=0.05)\n",
    "\n",
    "    S = risk_models.sample_cov(buy_stocks_history)\n",
    "\n",
    "    # Ensure that the adjusted_mu and S use the same tickers\n",
    "    common_tickers = adjusted_mu.index.intersection(S.index)\n",
    "\n",
    "    S = S.loc[common_tickers, common_tickers]\n",
    "    ef = EfficientFrontier(adjusted_mu, S)\n",
    "    ef.max_sharpe()\n",
    "    clean_weights = ef.clean_weights()\n",
    "\n",
    "    # Final portfolio weights\n",
    "    weights = OrderedDict()\n",
    "    for key, value in clean_weights.items():\n",
    "        if value != 0.0:\n",
    "            weights[key] = value\"\"\"\n",
    "    return recommendations, 0\n",
    "(recommendations, weights) = run_model(real_stock_portfolio_3) # <- Replace with the portfolio you want here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22859ca0fc7abba3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T05:44:15.902024Z",
     "start_time": "2025-08-12T05:44:15.893897Z"
    }
   },
   "outputs": [],
   "source": [
    "#recommendations.sort_values(\"Recommendation\", ascending=False, inplace=True)\n",
    "recommendations.to_csv(\"recommendations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379551e30428bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ideal Portfolio: \\n\")\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cca89-e09a-4a67-8137-b9dfaf1cf39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
